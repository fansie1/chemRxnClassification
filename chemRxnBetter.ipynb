{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fansie1/chemRxnClassification/blob/main/chemRxnBetter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rXp20TXLTcjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bdff37-4773-4c65-91b5-bb5c0e0f9439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.10)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.35)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.30.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install evaluate\n",
        "!pip install wandb\n",
        "!pip install sacremoses\n",
        "!pip install spacy\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!pip install seqeval\n",
        "\n",
        "import pdb\n",
        "import wandb\n",
        "use_wandb = False\n",
        "if use_wandb:\n",
        "  wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --no-cookies https://drive.google.com/uc?id=1--UFHJY3ldytNTsLcseIMDXEyzFeYEuj\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-oYW7JgHyhFj1HpbYvpHdsK8y-RwF2pW\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-KkDBrlyr3DF91BOTgp-vgCLwMe2JAmX"
      ],
      "metadata": {
        "id": "mSpXzf_ffUKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a0ac77-22f0-4eb9-c6d4-f441663cba26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--UFHJY3ldytNTsLcseIMDXEyzFeYEuj\n",
            "To: /content/train_optim_concat.txt\n",
            "100% 1.57M/1.57M [00:00<00:00, 115MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-oYW7JgHyhFj1HpbYvpHdsK8y-RwF2pW\n",
            "To: /content/test_optim_concat.txt\n",
            "100% 167k/167k [00:00<00:00, 115MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-KkDBrlyr3DF91BOTgp-vgCLwMe2JAmX\n",
            "To: /content/dev_optim_concat.txt\n",
            "100% 162k/162k [00:00<00:00, 84.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取数据集\n",
        "# train = \"/content/drive/MyDrive/chemrxnconfig/prod/train_optim_concat.txt\"\n",
        "# dev = \"/content/drive/MyDrive/chemrxnconfig/prod/dev_optim_concat.txt\"\n",
        "# test = \"/content/drive/MyDrive/chemrxnconfig/prod/test_optim_concat.txt\"\n",
        "train = \"/content/train_optim_concat.txt\"\n",
        "dev = \"/content/dev_optim_concat.txt\"\n",
        "test = \"/content/test_optim_concat.txt\"\n",
        "\n",
        "bert_tag_label_id = -100\n",
        "spacy_tag_label_id = -100\n",
        "pad_label_id = -100\n",
        "pad_token = 0\n",
        "prod_labels = [\"O\", \"B-Prod\", \"I-Prod\"]\n",
        "\n",
        "label2id = {context: i for i, context in enumerate(prod_labels)}\n",
        "id2label = {i: context for i, context in enumerate(prod_labels)}\n",
        "def read_examples_from_file(file_path):\n",
        "    guid_index = 1\n",
        "    examples = []\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        words, labels = [], []\n",
        "        metainfo = None\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if line.startswith(\"#\\tpassage\"):\n",
        "                metainfo = line\n",
        "            elif line == \"\":\n",
        "                if words:\n",
        "                    examples.append({\n",
        "                            \"guid\":f\"{guid_index}\",\n",
        "                            \"words\":words,\n",
        "                            \"metainfo\":metainfo,\n",
        "                            \"labels\":labels\n",
        "                        })\n",
        "                    guid_index += 1\n",
        "                    words, labels = [], []\n",
        "            else:\n",
        "                splits = line.split(\"\\t\")\n",
        "                words.append(splits[0])\n",
        "                if len(splits) > 1:\n",
        "                    labels.append(splits[-1])\n",
        "                else:\n",
        "                    # Examples could have no label for plain test files\n",
        "                    labels.append(\"O\")\n",
        "        if words:\n",
        "\n",
        "            examples.append({\n",
        "                    \"guid\":f\"{guid_index}\",\n",
        "                    \"words\":words,\n",
        "                    \"metainfo\":metainfo,\n",
        "                    \"labels\":labels\n",
        "                 })\n",
        "\n",
        "    return examples\n",
        "\n",
        "train_input = read_examples_from_file(train)\n",
        "dev_input = read_examples_from_file(dev)\n",
        "test_input = read_examples_from_file(test)"
      ],
      "metadata": {
        "id": "w_7F_rckJaPV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "spacy_model = spacy.load(\"en_core_web_sm\")\n",
        "en = English()\n",
        "\n",
        "checkpoint = \"jiangg/chembert_cased\"\n",
        "# checkpoint = \"dmis-lab/biobert-large-cased-v1.1-squad\"\n",
        "# checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "# checkpoint = \"microsoft/biogpt\"\n",
        "# spacy_pos_token = {\n",
        "#     \"ADJ\": \"[unused1]\",\n",
        "#     \"ADP\": \"[unused2]\",\n",
        "#     \"ADV\": \"[unused3]\",\n",
        "#     \"AUX\": \"[unused4]\",\n",
        "#     \"CCONJ\": \"[unused5]\",\n",
        "#     \"DET\": \"[unused6]\",\n",
        "#     \"INTJ\": \"[unused7]\",\n",
        "#     \"NOUN\": \"[unused8]\",\n",
        "#     \"NUM\": \"[unused9]\",\n",
        "#     \"PART\": \"[unused10]\",\n",
        "#     \"PRON\": \"[unused11]\",\n",
        "#     \"PROPN\": \"[unused12]\",\n",
        "#     \"PUNCT\": \"[unused13]\",\n",
        "#     \"SCONJ\": \"[unused14]\",\n",
        "#     \"SYM\": \"[unused15]\",\n",
        "#     \"VERB\": \"[unused16]\",\n",
        "#     \"X\": \"[unused17]\",\n",
        "# }\n",
        "\n",
        "# just useing the  noun and verb\n",
        "spacy_pos_token = {\"NOUN\": \"[unused8]\", \"VERB\": \"[unused16]\",}\n",
        "\n",
        "CLS = \"[CLS]\"\n",
        "SEP = \"[SEP]\"\n",
        "max_sentence = 256\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "token_cache_path = {\n",
        "    \"train\": \"token_cache_train\",\n",
        "    \"validation\": \"token_cache_validation\",\n",
        "    \"test\": \"token_cache_test\"\n",
        "}\n",
        "\n",
        "\n",
        "def build_spacy(sent_list):\n",
        "  return {\n",
        "      \"spacy\": spacy_model(' '.join(sent_list)),\n",
        "      \"find_idx\": build_sent_idx(sent_list)\n",
        "}\n",
        "\n",
        "\n",
        "def build_sent_idx(sent_list):\n",
        "  total_len = 0\n",
        "  find_idx = []\n",
        "  for i, word in enumerate(sent_list):\n",
        "    find_idx.append(str(total_len) + \"-\" + str(total_len + len(word) - 1))\n",
        "    # stridx2listidx[str(total_len) + \"-\" + str(total_len + len(word))] = i\n",
        "    total_len = total_len + len(word) + 1\n",
        "  return find_idx\n",
        "\n",
        "def do_find_idx(find_idx, idx):\n",
        "  for i, func in enumerate(find_idx):\n",
        "    func_list = func.split(\"-\")\n",
        "    if (int(func_list[0]) <= idx and int(func_list[1]) >= idx):\n",
        "      return i\n",
        "\n",
        "\n",
        "def check_append_word(tokens_list, word):\n",
        "  if (len(tokens_list) + 1) > max_sentence - 1:\n",
        "    return\n",
        "  tokens_list.append(word)"
      ],
      "metadata": {
        "id": "0fKFITsYRE3_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def tokenize_function(example):\n",
        "  word_list = example[\"words\"]\n",
        "  original_labels = example[\"labels\"]\n",
        "  spacy_result = build_spacy(word_list)\n",
        "  spacy = spacy_result[\"spacy\"]\n",
        "  sent_idx = spacy_result[\"find_idx\"]\n",
        "  tokens = [CLS]\n",
        "  labels = [bert_tag_label_id]\n",
        "  last_idx = 0\n",
        "  sub_token_tag = None\n",
        "  for _, word in enumerate(spacy):\n",
        "    original_idx = do_find_idx(sent_idx, word.idx)\n",
        "    # the sub token from same token\n",
        "    if original_idx == last_idx:\n",
        "      if not sub_token_tag and spacy_pos_token.get(word.pos_):\n",
        "        sub_token_tag = spacy_pos_token.get(word.pos_)\n",
        "    # the sub token from new token\n",
        "    else:\n",
        "      last_token = word_list[last_idx]\n",
        "      first_sub_token = True\n",
        "      for sub_token in tokenizer.tokenize(last_token):\n",
        "        check_append_word(tokens, sub_token)\n",
        "        label_tag = original_labels[last_idx] if first_sub_token else \"O\"\n",
        "        check_append_word(labels,  label2id[label_tag])\n",
        "        first_sub_token = False\n",
        "      # add verb or noun token\n",
        "      if sub_token_tag:\n",
        "        check_append_word(tokens, sub_token_tag)\n",
        "        check_append_word(labels, spacy_tag_label_id)\n",
        "      last_idx = original_idx\n",
        "  assert len(tokens) == len(labels)\n",
        "  input_ids, token_type_ids, attention_mask, labels = padding_fn(\n",
        "      tokenizer.convert_tokens_to_ids(tokens),\n",
        "      [0] * len(tokens),\n",
        "      [1] * len(tokens),\n",
        "      labels,\n",
        "  )\n",
        "  decoder_mask = [(x != pad_label_id) for x in labels]\n",
        "  assert len(input_ids) == max_sentence\n",
        "  assert len(token_type_ids) == max_sentence\n",
        "  assert len(attention_mask) == max_sentence\n",
        "  assert len(labels) == max_sentence\n",
        "  assert len(decoder_mask) == max_sentence\n",
        "  return {\n",
        "    \"input_ids\": input_ids,\n",
        "    \"token_type_ids\": token_type_ids,\n",
        "    \"attention_mask\": attention_mask,\n",
        "    \"labels\": labels,\n",
        "    \"decoder_mask\": decoder_mask\n",
        "  }\n",
        "\n",
        "\n",
        "def tokenize_function_no_spacy(example):\n",
        "  tokens = []\n",
        "  label_ids = []\n",
        "  word_list = example[\"words\"]\n",
        "  original_labels = example[\"labels\"]\n",
        "\n",
        "  for word, label in zip(word_list, original_labels):\n",
        "    word_tokens = tokenizer.tokenize(word)\n",
        "    if len(word_tokens) > 0:\n",
        "      tokens.extend(word_tokens)\n",
        "      label_ids.extend([label2id[label]] + [pad_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "  if len(tokens) > max_sentence - 2:\n",
        "    tokens = tokens[:(max_sentence - 2)]\n",
        "    label_ids = label_ids[:(max_sentence - 2)]\n",
        "\n",
        "  tokens += [SEP]\n",
        "  label_ids += [pad_label_id]\n",
        "\n",
        "  tokens = [CLS] + tokens\n",
        "  label_ids = [pad_label_id] + label_ids\n",
        "\n",
        "  input_ids, token_type_ids, attention_mask, labels = padding_fn(\n",
        "      tokenizer.convert_tokens_to_ids(tokens),\n",
        "      [0] * len(tokens),\n",
        "      [1] * len(tokens),\n",
        "      label_ids,\n",
        "  )\n",
        "  decoder_mask = [(x != pad_label_id) for x in label_ids]\n",
        "  assert len(input_ids) == max_sentence\n",
        "  assert len(token_type_ids) == max_sentence\n",
        "  assert len(attention_mask) == max_sentence\n",
        "  assert len(label_ids) == max_sentence\n",
        "  assert len(decoder_mask) == max_sentence\n",
        "  return {\n",
        "    \"input_ids\": input_ids,\n",
        "    \"token_type_ids\": token_type_ids,\n",
        "    \"attention_mask\": attention_mask,\n",
        "    \"labels\": label_ids,\n",
        "    \"decoder_mask\": decoder_mask\n",
        "  }\n",
        "\n",
        "def padding_fn(input_ids, token_type_ids, attention_mask, labels):\n",
        "  pad_len = max_sentence - len(input_ids)\n",
        "  input_ids += [pad_token] * pad_len\n",
        "  token_type_ids += [0] * pad_len\n",
        "  attention_mask += [0] * pad_len\n",
        "  labels += [pad_label_id] * pad_len\n",
        "  return input_ids, token_type_ids, attention_mask, labels\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "      checkpoint,\n",
        "      num_labels=3,\n",
        "      id2label=id2label,\n",
        "      label2id=label2id,\n",
        ")\n",
        "hidden_size=768\n",
        "num_train_epochs=20\n",
        "\n",
        "train_dataset = Dataset.from_list(train_input)\n",
        "dev_dataset = Dataset.from_list(dev_input)\n",
        "test_dataset = Dataset.from_list(test_input)\n",
        "\n",
        "datasetDict = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": dev_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n",
        "\n",
        "\n",
        "tokenized_datasets = datasetDict.map(function=tokenize_function_no_spacy,\n",
        "                      cache_file_names=token_cache_path,\n",
        "                      load_from_cache_file=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"words\", \"guid\", \"metainfo\"])\n",
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "ynkYdSTNUqks"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding, DataCollatorForTokenClassification, default_data_collator\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 16\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True,  batch_size=batch_size, collate_fn=default_data_collator)\n",
        "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size, collate_fn=default_data_collator)\n",
        "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=default_data_collator)"
      ],
      "metadata": {
        "id": "HdBIFE_p9GzM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class FocalLossSelf(nn.Module):\n",
        "    r\"\"\"\n",
        "        This criterion is a implemenation of Focal Loss, which is proposed in\n",
        "        Focal Loss for Dense Object Detection.\n",
        "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
        "        The losses are averaged across observations for each minibatch.\n",
        "        Args:\n",
        "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
        "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5),\n",
        "                                   putting more focus on hard, misclassiﬁed examples\n",
        "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
        "                                However, if the field size_average is set to False, the losses are\n",
        "                                instead summed for each minibatch.\n",
        "    \"\"\"\n",
        "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        if alpha is None:\n",
        "            self.alpha = Variable(torch.ones(class_num, 1))\n",
        "        else:\n",
        "            if isinstance(alpha, Variable):\n",
        "                self.alpha = alpha\n",
        "            else:\n",
        "                self.alpha = Variable(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.class_num = class_num\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        N = inputs.size(0)\n",
        "        C = inputs.size(1)\n",
        "        P = F.softmax(inputs)\n",
        "\n",
        "        class_mask = inputs.data.new(N, C).fill_(0)\n",
        "        class_mask = Variable(class_mask)\n",
        "        ids = targets.view(-1, 1)\n",
        "        class_mask.scatter_(1, ids.data, 1.)\n",
        "        #print(class_mask)\n",
        "\n",
        "\n",
        "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
        "            self.alpha = self.alpha.cuda()\n",
        "        alpha = self.alpha[ids.data.view(-1)]\n",
        "\n",
        "        probs = (P*class_mask).sum(1).view(-1,1)\n",
        "\n",
        "        log_p = probs.log()\n",
        "        #print('probs size= {}'.format(probs.size()))\n",
        "        #print(probs)\n",
        "\n",
        "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p\n",
        "        #print('-----bacth_loss------')\n",
        "        #print(batch_loss)\n",
        "\n",
        "\n",
        "        if self.size_average:\n",
        "            loss = batch_loss.mean()\n",
        "        else:\n",
        "            loss = batch_loss.sum()\n",
        "        return loss\n",
        "\n",
        "class FocalLoss(nn.CrossEntropyLoss):\n",
        "    ''' Focal loss for classification tasks on imbalanced datasets '''\n",
        "\n",
        "    def __init__(self, gamma=2, alpha=None, ignore_index=-100):\n",
        "        super().__init__(weight=alpha, ignore_index=ignore_index)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, input_, target):\n",
        "        cross_entropy = super().forward(input_, target)\n",
        "        # Temporarily mask out ignore index to '0' for valid gather-indices input.\n",
        "        # This won't contribute final loss as the cross_entropy contribution\n",
        "        # for these would be zero.\n",
        "        target = target * (target != self.ignore_index).long()\n",
        "        input_prob = torch.gather(F.softmax(input_, 1), 1, target.unsqueeze(1))\n",
        "        loss = torch.pow(1 - input_prob, self.gamma) * cross_entropy\n",
        "        return torch.mean(loss)"
      ],
      "metadata": {
        "id": "wsrXTVW9z0p9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertForTokenClassification, TrainingArguments, AutoConfig\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "class BertForTagging(BertForTokenClassification):\n",
        "    def __init__(self, config):\n",
        "        super(BertForTagging, self).__init__(config)\n",
        "        # self.classifier = nn.Linear(hidden_size, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        outputs = (logits,)\n",
        "\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        # loss_fct = FocalLoss(alpha = torch.tensor([0.001, 0.4995, 0.4995], device=device))\n",
        "        active_loss = attention_mask.view(-1) == 1\n",
        "        active_logits = logits.view(-1, self.num_labels)\n",
        "        active_labels = torch.where(\n",
        "            active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "        )\n",
        "        loss = loss_fct(active_logits, active_labels)\n",
        "        outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "    def decode(self, logits, mask):\n",
        "        preds = torch.argmax(logits, dim=2).cpu().numpy()\n",
        "        batch_size, seq_len = preds.shape\n",
        "        preds_list = [[] for _ in range(batch_size)]\n",
        "        for i in range(batch_size):\n",
        "            for j in range(seq_len):\n",
        "                if mask[i, j]:\n",
        "                    preds_list[i].append(preds[i,j])\n",
        "        return preds_list\n",
        "\n",
        "    def decode_labels(self, labels, mask):\n",
        "        labels = labels.cpu().numpy()\n",
        "        batch_size, seq_len = labels.shape\n",
        "        labels_list = [[] for _ in range(batch_size)]\n",
        "        for i in range(batch_size):\n",
        "          for j in range(seq_len):\n",
        "            if mask[i, j]:\n",
        "              labels_list[i].append(labels[i, j])\n",
        "        return labels_list\n",
        "\n"
      ],
      "metadata": {
        "id": "FMXLaNZFTjt9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and validation\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "from seqeval.metrics import f1_score as f1, precision_score as precision, recall_score as recall\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "import wandb\n",
        "\n",
        "use_wandb = False\n",
        "if use_wandb:\n",
        "  wandb.login()\n",
        "\n",
        "\n",
        "if use_wandb:\n",
        "  wandb.init(project=\"rxn_better_train\")\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(predictions, label_ids):\n",
        "    label_list_str = [[id2label[x] for x in seq] for seq in label_ids]\n",
        "    preds_list_str = [[id2label[x] for x in seq] for seq in predictions]\n",
        "    preds_list = [j for i in predictions for j in i]\n",
        "    label_list = [j for i in label_ids for j in i]\n",
        "    # pdb.set_trace()\n",
        "    return {\n",
        "        \"precision_score\": precision_score(label_list, preds_list, average='micro'),\n",
        "        \"recall_score\": recall_score(label_list, preds_list, average='micro'),\n",
        "        \"f1_score\": f1_score(label_list, preds_list, average='micro'),\n",
        "        \"precision\": precision(label_list_str, preds_list_str),\n",
        "        \"recall\": recall(label_list_str, preds_list_str),\n",
        "        \"f1\": f1(label_list_str, preds_list_str),\n",
        "    }\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, scheduler, device, accumulation_steps=1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    # pdb.set_trace()\n",
        "    with tqdm(enumerate(data_loader), unit=\"batch\", total=len(data_loader)) as tepoch:\n",
        "        for batch_index, dataset in tepoch:\n",
        "            tepoch.set_description(f\"Epoch Started\")\n",
        "\n",
        "            input_ids = dataset['input_ids']\n",
        "            attention_mask = dataset['attention_mask']\n",
        "            token_type_ids = dataset['token_type_ids']\n",
        "            labels = dataset['labels']\n",
        "\n",
        "            input_ids = input_ids.to(device, dtype = torch.long)\n",
        "            attention_mask = attention_mask.to(device, dtype = torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype = torch.long)\n",
        "            labels = labels.to(device, dtype = torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                labels=labels\n",
        "                )\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "            decoder_mask = dataset['decoder_mask']\n",
        "            decode_predictions = model.decode(outputs[1], decoder_mask)\n",
        "            decode_labels = model.decode_labels(labels, decoder_mask)\n",
        "            metrics_result = compute_metrics(decode_predictions, decode_labels)\n",
        "            # pdb.set_trace()\n",
        "            train_accuracy = 100.0 * metrics_result[\"precision\"]\n",
        "            # recall = 100.0 * metrics_result[\"recall\"]\n",
        "            # f1 = 100.0 * metrics_result[\"f1\"]\n",
        "            # train_accuracy = 100.0 * calculate_accuracy(outputs[1], labels, attention_mask)\n",
        "            tepoch.set_postfix(loss=loss.item(), accuracy=train_accuracy)\n",
        "            if use_wandb:\n",
        "              wandb.log({\"train_accuracy\": train_accuracy, \"train_loss\": loss})\n",
        "            if (batch_index+1) % accumulation_steps == 0 :\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += train_accuracy\n",
        "\n",
        "    return total_loss/len(data_loader), total_accuracy/len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "\n",
        "    final_labels = []\n",
        "    final_pred = []\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for _, dataset in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            input_ids = dataset['input_ids']\n",
        "            attention_mask = dataset['attention_mask']\n",
        "            token_type_ids = dataset['token_type_ids']\n",
        "            labels = dataset['labels']\n",
        "            decoder_mask = dataset['decoder_mask']\n",
        "\n",
        "            input_ids = input_ids.to(device, dtype = torch.long)\n",
        "            attention_mask = attention_mask.to(device, dtype = torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype = torch.long)\n",
        "            labels = labels.to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs[0]\n",
        "            loss += loss.item()\n",
        "            decode_predictions = model.decode(outputs[1], decoder_mask)\n",
        "            decode_labels = model.decode_labels(labels, decoder_mask)\n",
        "            final_labels.extend(decode_labels)\n",
        "            final_pred.extend(decode_predictions)\n",
        "\n",
        "    return loss/len(data_loader), final_pred, final_labels\n",
        "\n",
        "\n",
        "def calculate_accuracy(outputs, labels, mask, type_acr = 'batch'):\n",
        "    if type_acr == 'batch':\n",
        "        # 从GPU中脱离出来并且返回一个全新的不带梯度的tensor，然后转成numpy\n",
        "        mask = mask.cpu().detach().numpy()\n",
        "        labels = labels.cpu().detach().numpy()[mask == 1]\n",
        "        outputs = outputs.cpu().detach().numpy()[mask == 1]\n",
        "        # 返回参数最大的下表  axis=a 表示以a维来统计\n",
        "        predictions = np.argmax(outputs, axis=1)\n",
        "        return metrics.accuracy_score(labels.tolist(), predictions.tolist())\n",
        "    return metrics.accuracy_score(labels, outputs)"
      ],
      "metadata": {
        "id": "NQCcEd-8UdP8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "from evaluate import evaluator\n",
        "\n",
        "model = BertForTagging(config)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-4},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0},\n",
        "]\n",
        "\n",
        "num_training_steps = int(len(train_dataloader)/batch_size * num_train_epochs)\n",
        "\n",
        "optimizer = AdamW(optimizer_parameters, lr=1e-3)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = 0,\n",
        "    num_training_steps = num_training_steps\n",
        ")\n",
        "\n",
        "def validate_model(model, dataloader):\n",
        "  model.eval()\n",
        "  # metrics = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
        "  metrics = evaluator(\"token-classification\")\n",
        "  for batch in dataloader:\n",
        "    batch = {k:v.to(device) for k,v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "      output = model(**batch)\n",
        "    logits = output[1]\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    labels = batch[\"labels\"]\n",
        "    ids = batch[\"input_ids\"]\n",
        "    assert len(predictions) == len(labels)\n",
        "    # for pred, lab, token in zip(predictions, labels, ids):\n",
        "    #   if (pred != lab):\n",
        "    #     write_txt(\"/result.txt\", tokenizer.decode(token), pred, lab)\n",
        "    # pdb.set_trace()\n",
        "    metrics.add_batch(predictions=predictions.view(-1), references=labels.view(-1))\n",
        "  return metrics.compute()\n",
        "\n",
        "for e in range(num_train_epochs):\n",
        "  print(\"Epoch:\", e)\n",
        "  loss, train_accuracy = train_fn(train_dataloader, model, optimizer, scheduler, device)\n",
        "  print(f\"Total Epoch Train Accuracy : {train_accuracy} with loss : {loss}\")\n",
        "  loss, predicted, labels = eval_fn(eval_dataloader, model, device)\n",
        "  result = compute_metrics(predicted, labels)\n",
        "  print(\"----------validate----------\")\n",
        "  print(f\"Total Epoch Eval : {100.0 *  result['precision']}, Recall : {100.0 * result['recall']}, f1 : {100.0 * result['f1']}\")\n",
        "  loss, predicted, labels = eval_fn(test_dataloader, model, device)\n",
        "  result = compute_metrics(predicted, labels)\n",
        "  print(\"----------test----------\")\n",
        "  print(f\"Total Epoch Test : {100.0 *  result['precision']}, Recall : {100.0 * result['recall']}, f1 : {100.0 * result['f1']}\")\n",
        "  # validate_model(model, eval_dataloader)\n",
        "  # print(\"----------test----------\")\n",
        "  # validate_model(model, test_dataloader)"
      ],
      "metadata": {
        "id": "C0owFZ0CTomf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "acd3dc34-cdf3-4f59-bcf0-c3bcc52a65ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch Started:   0%|          | 1/386 [00:02<17:50,  2.78s/batch, accuracy=0.806, loss=1.48]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   1%|          | 2/386 [00:03<10:33,  1.65s/batch, accuracy=0, loss=0.241]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   1%|          | 4/386 [00:05<06:37,  1.04s/batch, accuracy=1.35, loss=5.43]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   1%|▏         | 5/386 [00:05<05:56,  1.07batch/s, accuracy=0, loss=0.0914]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   2%|▏         | 7/386 [00:07<05:13,  1.21batch/s, accuracy=0, loss=16.3]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   2%|▏         | 8/386 [00:08<05:00,  1.26batch/s, accuracy=0, loss=0.224]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   2%|▏         | 9/386 [00:08<04:51,  1.29batch/s, accuracy=0, loss=0.103]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   3%|▎         | 10/386 [00:09<04:49,  1.30batch/s, accuracy=0, loss=0.129]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   3%|▎         | 11/386 [00:10<04:44,  1.32batch/s, accuracy=0, loss=0.18]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   3%|▎         | 12/386 [00:11<04:41,  1.33batch/s, accuracy=0, loss=0.438]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   3%|▎         | 13/386 [00:11<04:36,  1.35batch/s, accuracy=0, loss=0.0682]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   4%|▎         | 14/386 [00:12<04:48,  1.29batch/s, accuracy=0, loss=0.109]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   4%|▍         | 16/386 [00:14<04:34,  1.35batch/s, accuracy=0, loss=0.446]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   4%|▍         | 17/386 [00:14<04:23,  1.40batch/s, accuracy=0, loss=0.0727]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   5%|▍         | 18/386 [00:15<04:18,  1.42batch/s, accuracy=0, loss=0.0892]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   5%|▍         | 19/386 [00:16<04:15,  1.44batch/s, accuracy=0, loss=0.106]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   5%|▌         | 20/386 [00:16<04:13,  1.44batch/s, accuracy=0, loss=0.27]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   5%|▌         | 21/386 [00:17<04:11,  1.45batch/s, accuracy=0, loss=0.202]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(\n",
            "Epoch Started:   6%|▌         | 22/386 [00:18<04:09,  1.46batch/s, accuracy=0, loss=0.000258]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   6%|▌         | 23/386 [00:18<04:08,  1.46batch/s, accuracy=0, loss=0.195]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   6%|▌         | 24/386 [00:19<04:08,  1.46batch/s, accuracy=0, loss=0.173]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   6%|▋         | 25/386 [00:20<04:03,  1.48batch/s, accuracy=0, loss=0.166]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   7%|▋         | 26/386 [00:20<04:00,  1.50batch/s, accuracy=0, loss=0.209]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   7%|▋         | 27/386 [00:21<03:58,  1.51batch/s, accuracy=0, loss=0.0589]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   7%|▋         | 28/386 [00:22<03:55,  1.52batch/s, accuracy=0, loss=0.0854]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   8%|▊         | 29/386 [00:22<03:52,  1.53batch/s, accuracy=0, loss=0.294]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   8%|▊         | 30/386 [00:23<03:53,  1.52batch/s, accuracy=0, loss=0.275]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   8%|▊         | 31/386 [00:24<03:53,  1.52batch/s, accuracy=0, loss=0.0902]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   8%|▊         | 32/386 [00:24<03:52,  1.52batch/s, accuracy=0, loss=0.145]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   9%|▊         | 33/386 [00:25<03:51,  1.52batch/s, accuracy=0, loss=0.332]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   9%|▉         | 34/386 [00:26<03:51,  1.52batch/s, accuracy=0, loss=0.149]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   9%|▉         | 35/386 [00:26<03:50,  1.52batch/s, accuracy=0, loss=0.133]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   9%|▉         | 36/386 [00:27<03:50,  1.52batch/s, accuracy=0, loss=0.14]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Epoch Started:   9%|▉         | 36/386 [00:27<04:32,  1.29batch/s, accuracy=0, loss=0.11]\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-10-98926cccf230>\", line 46, in <cell line: 44>\n",
            "    loss, train_accuracy = train_fn(train_dataloader, model, optimizer, scheduler, device)\n",
            "  File \"<ipython-input-9-b25602e54aae>\", line 76, in train_fn\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\", line 69, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 280, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py\", line 476, in step\n",
            "    p.addcdiv_(exp_avg, denom, value=-step_size)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 397, in realpath\n",
            "    return abspath(path)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-98926cccf230>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total Epoch Train Accuracy : {train_accuracy} with loss : {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b25602e54aae>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, scheduler, device, accumulation_steps)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# validate_model(model, eval_dataloader)"
      ],
      "metadata": {
        "id": "inhjbrnLMopp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}